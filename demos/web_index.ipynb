{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_key = api_key\n",
    "base_url = \"http://0.0.0.0:8000\" \n",
    "openai.base_url =base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import textwrap\n",
    "from typing import List, Union\n",
    "\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    VectorStoreIndex\n",
    ")\n",
    "from llama_index.callbacks import CallbackManager, OpenInferenceCallbackHandler\n",
    "from llama_index.callbacks.open_inference_callback import (\n",
    "    as_dataframe,\n",
    "    QueryData,\n",
    "    NodeData,\n",
    ")\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "SimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\n",
    "\n",
    "loader = SimpleWebPageReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetCallback:\n",
    "    def __init__(\n",
    "        self, data_path: Union[str, Path], max_buffer_length: int = 1000\n",
    "    ):\n",
    "        self._data_path = Path(data_path)\n",
    "        self._data_path.mkdir(parents=True, exist_ok=False)\n",
    "        self._max_buffer_length = max_buffer_length\n",
    "        self._batch_index = 0\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        query_data_buffer: List[QueryData],\n",
    "        node_data_buffer: List[NodeData],\n",
    "    ) -> None:\n",
    "        if len(query_data_buffer) > self._max_buffer_length:\n",
    "            query_dataframe = as_dataframe(query_data_buffer)\n",
    "            file_path = self._data_path / f\"log-{self._batch_index}.parquet\"\n",
    "            query_dataframe.to_parquet(file_path)\n",
    "            self._batch_index += 1\n",
    "            query_data_buffer.clear()  # ⚠️ clear the buffer or it will keep growing forever!\n",
    "            node_data_buffer.clear() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "query_dataframes = []\n",
    "data_path = \"../prototype-search-application/resources/parquet/demo-graz.parquet\"\n",
    "\n",
    "df = pd.read_parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['warc_date']=df['warc_date'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "from llama_index.schema import Document\n",
    "\n",
    "docs = []\n",
    "for index, row in df.iterrows():\n",
    "    metadata = dict(row)\n",
    "    metadata['plain_text'] = \"\"\n",
    "    docs.append(Document(doc_id= row[\"id\"], text=row[\"plain_text\"],metadata=metadata ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown model 'mistral'. Please provide a valid OpenAI model name in: gpt-4, gpt-4-32k, gpt-4-1106-preview, gpt-4-vision-preview, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-1106, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002, gpt-3.5-turbo-instruct, text-ada-001, text-babbage-001, text-curie-001, ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager([callback_handler])\n\u001b[1;32m      6\u001b[0m llm_predictor \u001b[38;5;241m=\u001b[39m LLMPredictor(llm\u001b[38;5;241m=\u001b[39mChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral\u001b[39m\u001b[38;5;124m\"\u001b[39m,api_key\u001b[38;5;241m=\u001b[39mapi_key,base_url\u001b[38;5;241m=\u001b[39mbase_url))\n\u001b[0;32m----> 7\u001b[0m service_context \u001b[38;5;241m=\u001b[39m \u001b[43mServiceContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_predictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     12\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemo-graz.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/service_context.py:184\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, text_splitter, transformations, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    180\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m resolve_embed_model(embed_model)\n\u001b[1;32m    181\u001b[0m embed_model\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n\u001b[1;32m    183\u001b[0m prompt_helper \u001b[38;5;241m=\u001b[39m prompt_helper \u001b[38;5;129;01mor\u001b[39;00m _get_default_prompt_helper(\n\u001b[0;32m--> 184\u001b[0m     llm_metadata\u001b[38;5;241m=\u001b[39m\u001b[43mllm_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m,\n\u001b[1;32m    185\u001b[0m     context_window\u001b[38;5;241m=\u001b[39mcontext_window,\n\u001b[1;32m    186\u001b[0m     num_output\u001b[38;5;241m=\u001b[39mnum_output,\n\u001b[1;32m    187\u001b[0m )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_splitter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m node_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify both text_splitter and node_parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/llm_predictor/base.py:148\u001b[0m, in \u001b[0;36mLLMPredictor.metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetadata\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMMetadata:\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get LLM metadata.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/llms/langchain.py:49\u001b[0m, in \u001b[0;36mLangChainLLM.metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetadata\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMMetadata:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlangchain_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_llm_metadata\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_llm_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/llms/langchain_utils.py:128\u001b[0m, in \u001b[0;36mget_llm_metadata\u001b[0;34m(llm)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMMetadata(\n\u001b[1;32m    121\u001b[0m         context_window\u001b[38;5;241m=\u001b[39manyscale_modelname_to_contextsize(llm\u001b[38;5;241m.\u001b[39mmodel_name),\n\u001b[1;32m    122\u001b[0m         num_output\u001b[38;5;241m=\u001b[39mllm\u001b[38;5;241m.\u001b[39mmax_tokens \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    123\u001b[0m         is_chat_model\u001b[38;5;241m=\u001b[39mis_chat_model_,\n\u001b[1;32m    124\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, ChatOpenAI):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMMetadata(\n\u001b[0;32m--> 128\u001b[0m         context_window\u001b[38;5;241m=\u001b[39m\u001b[43mopenai_modelname_to_contextsize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    129\u001b[0m         num_output\u001b[38;5;241m=\u001b[39mllm\u001b[38;5;241m.\u001b[39mmax_tokens \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    130\u001b[0m         is_chat_model\u001b[38;5;241m=\u001b[39mis_chat_model_,\n\u001b[1;32m    131\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m    132\u001b[0m     )\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, Cohere):\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# June 2023: Cohere's supported max input size for Generation models is 2048\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Reference: <https://docs.cohere.com/docs/tokens>\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMMetadata(\n\u001b[1;32m    137\u001b[0m         context_window\u001b[38;5;241m=\u001b[39mCOHERE_CONTEXT_WINDOW,\n\u001b[1;32m    138\u001b[0m         num_output\u001b[38;5;241m=\u001b[39mllm\u001b[38;5;241m.\u001b[39mmax_tokens,\n\u001b[1;32m    139\u001b[0m         is_chat_model\u001b[38;5;241m=\u001b[39mis_chat_model_,\n\u001b[1;32m    140\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    141\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/llms/openai_utils.py:190\u001b[0m, in \u001b[0;36mopenai_modelname_to_contextsize\u001b[0;34m(modelname)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has been discontinued. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease choose another model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modelname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ALL_AVAILABLE_MODELS:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. Please provide a valid OpenAI model name in:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ALL_AVAILABLE_MODELS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ALL_AVAILABLE_MODELS[modelname]\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown model 'mistral'. Please provide a valid OpenAI model name in: gpt-4, gpt-4-32k, gpt-4-1106-preview, gpt-4-vision-preview, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-1106, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002, gpt-3.5-turbo-instruct, text-ada-001, text-babbage-001, text-curie-001, ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index import LLMPredictor, ServiceContext,StorageContext,load_index_from_storage\n",
    "\n",
    "callback_handler = OpenInferenceCallbackHandler()\n",
    "callback_manager = CallbackManager([callback_handler])\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.9, model_name=\"mistral\",api_key=api_key,base_url=base_url))\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    callback_manager=callback_manager\n",
    ")\n",
    "import re\n",
    "path = \"demo-graz.parquet\"\n",
    "folder_name = re.sub('[^A-Za-z0-9]+', '_', path.strip())\n",
    "\n",
    "try:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir= f\"../storage/index/{folder_name}\")\n",
    "    graz_index = load_index_from_storage(storage_context)\n",
    "    index_loaded = True\n",
    "    print(\"Index loaded.\")\n",
    "except:\n",
    "    index_loaded = False\n",
    "    \n",
    "if not index_loaded:\n",
    "    parser = SimpleNodeParser.from_defaults(chunk_size=1024)\n",
    "    nodes = parser.get_nodes_from_documents(docs,show_progress=True)\n",
    "    print(\"Building index...\")\n",
    "    graz_index = VectorStoreIndex(\n",
    "        nodes, service_context=service_context,show_progress=True\n",
    "    )\n",
    "    graz_index.storage_context.persist(persist_dir= f\"../storage/index/{folder_name}\")\n",
    "\n",
    "\n",
    "from llama_index import (\n",
    "    Prompt,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "response_template = \"\"\"\n",
    "## Question\n",
    "\n",
    "{question}\n",
    "\n",
    "\n",
    "## Answer\n",
    "```\n",
    "{response}\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "template = (\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, please answer the question: {query_str}\\n\"\n",
    "    \"Find information from different urls. \\n\"\n",
    "    \"Find top 10 distinct related url which related to query. Rename field as results. \\n\"\n",
    "    \"Find some webp image urls among related urls. Name field as image_url. \\n\"\n",
    "    \"Reduce each document to url and title and one small sentence. Rename snippet to sentence. \\n\"\n",
    "    \"Result should includes url, tittle, sentence.\"\n",
    "    \"Return the response in json format.\\n\"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "response_synthesizer = get_response_synthesizer(text_qa_template= qa_template)\n",
    "graz_engine = graz_index.as_query_engine(similarity_top_k = 3,response_synthesizer=response_synthesizer)\n",
    "\n",
    "def query(query: str, query_engine=graz_engine):\n",
    "    \n",
    "    response_md = query_engine.query(query)\n",
    "    display(Markdown(response_template.format(\n",
    "        question=query,\n",
    "        response=response_md,\n",
    "    \n",
    "    )))\n",
    "    return json.loads(str(response_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Question\n",
       "\n",
       "'Graz'\n",
       "\n",
       "\n",
       "## Answer\n",
       "```\n",
       "{\n",
       "  \"results\": [\n",
       "    {\n",
       "      \"url\": \"https://en.wikipedia.org/wiki/Graz\",\n",
       "      \"title\": \"Graz - Wikipedia\",\n",
       "      \"sentence\": \"Graz is the capital city of the Austrian state of Styria and second-largest city in Austria after Vienna.\"\n",
       "    },\n",
       "    {\n",
       "      \"url\": \"https://www.graz.at/\",\n",
       "      \"title\": \"City of Graz\",\n",
       "      \"sentence\": \"Official website of the city of Graz.\"\n",
       "    },\n",
       "    {\n",
       "      \"url\": \"https://www.graztourismus.at/en\",\n",
       "      \"title\": \"Graz Tourism\",\n",
       "      \"sentence\": \"Official tourism website of Graz.\"\n",
       "    },\n",
       "    {\n",
       "      \"url\": \"https://www.britannica.com/place/Graz\",\n",
       "      \"title\": \"Graz - Encyclopedia Britannica\",\n",
       "      \"sentence\": \"Graz is the second largest city in Austria and the capital of the Bundesland (federal state) of Styria.\"\n",
       "    },\n",
       "    {\n",
       "      \"url\": \"https://www.lonelyplanet.com/austria/styria/graz\",\n",
       "      \"title\": \"Graz - Lonely Planet\",\n",
       "      \"sentence\": \"Graz is a vibrant city in the heart of Styria, known for its rich history and beautiful architecture.\"\n",
       "    },\n",
       "    {\n",
       "      \"url\": \"https://www.tripadvisor.com/Tourism-g190432-Graz_Styria-Vacations.html\",\n",
       "      \"title\": \"Graz - TripAdvisor\",\n",
       "      \"sentence\": \"Plan your visit to Graz, Austria with the help of TripAdvisor's traveler reviews and photos.\"\n",
       "    },\n",
       "    {\n",
       "      \"url\": \"https://www.austria.info/us/where-to-go/cities/graz\",\n",
       "      \"title\": \"Graz - Austria.info\",\n",
       "      \"sentence\": \"Discover the city of Graz, a UNESCO World Heritage site, with its historic center and Schloss Eggenberg.\"\n",
       "    },\n",
       "    {\n",
       "      \"url\": \"https://www.graz-airport.com/\",\n",
       "      \"title\": \"Graz Airport\",\n",
       "      \"sentence\": \"Official website of Graz Airport, providing information on flights, services, and facilities.\"\n",
       "    },\n",
       "    {\n",
       "      \"url\": \"https://www.grazmuseum.at/en/\",\n",
       "      \"title\": \"Graz Museum\",\n",
       "      \"sentence\": \"Explore the history and culture of Graz at the Graz Museum.\"\n",
       "    },\n",
       "    {\n",
       "      \"url\": \"https://www.graz-kulturjahr2020.at/en/\",\n",
       "      \"title\": \"Graz Cultural Capital 2020\",\n",
       "      \"sentence\": \"Learn about the cultural events and activities happening in Graz as the Cultural Capital of Europe in 2020.\"\n",
       "    }\n",
       "  ],\n",
       "  \"image_url\": [\n",
       "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Innere_Stadt%2C_8010_Graz%2C_Austria_-_panoramio_%2822%29.jpg/800px-Innere_Stadt%2C_8010_Graz%2C_Austria_-_panoramio_%2822%29.jpg\",\n",
       "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/Innere_Stadt%2C_8010_Graz%2C_Austria_-_panoramio_%2814%29.jpg/800px-Innere_Stadt%2C_8010_Graz%2C_Austria_-_panoramio_%2814%29.jpg\",\n",
       "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/19-06-14-Graz-Murinsel-Schlo%C3%9Fberg-RalfR.jpg/800px-19-06-14-Graz-Murinsel-Schlo%C3%9Fberg-RalfR.jpg\",\n",
       "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Catedrala_Sf._Egidiu_din_Graz3.jpg/800px-Catedrala_Sf._Egidiu_din_Graz3.jpg\",\n",
       "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Katharinenkirche%2C_Graz.jpg/800px-Katharinenkirche%2C_Graz.jpg\"\n",
       "  ]\n",
       "}\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'results': [{'url': 'https://en.wikipedia.org/wiki/Graz',\n",
       "   'title': 'Graz - Wikipedia',\n",
       "   'sentence': 'Graz is the capital city of the Austrian state of Styria and second-largest city in Austria after Vienna.'},\n",
       "  {'url': 'https://www.graz.at/',\n",
       "   'title': 'City of Graz',\n",
       "   'sentence': 'Official website of the city of Graz.'},\n",
       "  {'url': 'https://www.graztourismus.at/en',\n",
       "   'title': 'Graz Tourism',\n",
       "   'sentence': 'Official tourism website of Graz.'},\n",
       "  {'url': 'https://www.britannica.com/place/Graz',\n",
       "   'title': 'Graz - Encyclopedia Britannica',\n",
       "   'sentence': 'Graz is the second largest city in Austria and the capital of the Bundesland (federal state) of Styria.'},\n",
       "  {'url': 'https://www.lonelyplanet.com/austria/styria/graz',\n",
       "   'title': 'Graz - Lonely Planet',\n",
       "   'sentence': 'Graz is a vibrant city in the heart of Styria, known for its rich history and beautiful architecture.'},\n",
       "  {'url': 'https://www.tripadvisor.com/Tourism-g190432-Graz_Styria-Vacations.html',\n",
       "   'title': 'Graz - TripAdvisor',\n",
       "   'sentence': \"Plan your visit to Graz, Austria with the help of TripAdvisor's traveler reviews and photos.\"},\n",
       "  {'url': 'https://www.austria.info/us/where-to-go/cities/graz',\n",
       "   'title': 'Graz - Austria.info',\n",
       "   'sentence': 'Discover the city of Graz, a UNESCO World Heritage site, with its historic center and Schloss Eggenberg.'},\n",
       "  {'url': 'https://www.graz-airport.com/',\n",
       "   'title': 'Graz Airport',\n",
       "   'sentence': 'Official website of Graz Airport, providing information on flights, services, and facilities.'},\n",
       "  {'url': 'https://www.grazmuseum.at/en/',\n",
       "   'title': 'Graz Museum',\n",
       "   'sentence': 'Explore the history and culture of Graz at the Graz Museum.'},\n",
       "  {'url': 'https://www.graz-kulturjahr2020.at/en/',\n",
       "   'title': 'Graz Cultural Capital 2020',\n",
       "   'sentence': 'Learn about the cultural events and activities happening in Graz as the Cultural Capital of Europe in 2020.'}],\n",
       " 'image_url': ['https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Innere_Stadt%2C_8010_Graz%2C_Austria_-_panoramio_%2822%29.jpg/800px-Innere_Stadt%2C_8010_Graz%2C_Austria_-_panoramio_%2822%29.jpg',\n",
       "  'https://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/Innere_Stadt%2C_8010_Graz%2C_Austria_-_panoramio_%2814%29.jpg/800px-Innere_Stadt%2C_8010_Graz%2C_Austria_-_panoramio_%2814%29.jpg',\n",
       "  'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/19-06-14-Graz-Murinsel-Schlo%C3%9Fberg-RalfR.jpg/800px-19-06-14-Graz-Murinsel-Schlo%C3%9Fberg-RalfR.jpg',\n",
       "  'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Catedrala_Sf._Egidiu_din_Graz3.jpg/800px-Catedrala_Sf._Egidiu_din_Graz3.jpg',\n",
       "  'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Katharinenkirche%2C_Graz.jpg/800px-Katharinenkirche%2C_Graz.jpg']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = query(\"'Graz'\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Unterminated string starting at: line 8 column 20 (char 271)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m query(\u001b[39m\"\u001b[39;49m\u001b[39mGraz airport\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m founded_webpages \u001b[39m=\u001b[39m[]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m final_results \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m response_md \u001b[39m=\u001b[39m query_engine\u001b[39m.\u001b[39mquery(query)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# display(Markdown(response_template.format(\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#     question=query,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#     response=response_md,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# )))\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39;49mloads(\u001b[39mstr\u001b[39;49m(response_md))[\u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 8 column 20 (char 271)"
     ]
    }
   ],
   "source": [
    "data = query(\"Graz airport\")\n",
    "founded_webpages =[]\n",
    "final_results = []\n",
    "for r in data:\n",
    "    if r[\"url\"] not in founded_webpages:\n",
    "        founded_webpages.append(r[\"url\"])\n",
    "        final_results.append(r)\n",
    "\n",
    "markdown_content = \"\"\n",
    "for item in final_results:\n",
    "    markdown_content += f\"[{item['title']}]({item['url']})\\n- {item['sentence']}\\n\\n\"\n",
    "\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.graztourismus.at/en/sightseeing-culture/sights/stiegenkirche_shg_6987',\n",
       "  'id': '<urn:uuid:516fbad4-ab1b-4b0d-b159-4c4a46d7c6e8>',\n",
       "  'title': 'No Tittle',\n",
       "  'language': 'en',\n",
       "  'warcDate': '1684768605000000',\n",
       "  'wordCount': 184,\n",
       "  'sentence': 'The oldest parish church in Graz was first mentioned in a historical document in 1343. Known chiefly as a church for students, it lies in the historic centre of Graz close up against the southern side of the Schlossberg,concealed behind the heavy walls of a former Augustinian monastery.'},\n",
       " {'url': 'https://www.graztourismus.at/en/sightseeing-culture/sights/generalihof_shg_6958',\n",
       "  'id': '<urn:uuid:9907615a-d4be-491c-8c5d-ab60fc8bee1d>',\n",
       "  'title': 'No Tittle',\n",
       "  'language': 'en',\n",
       "  'warcDate': '1684768609000000',\n",
       "  'wordCount': 195,\n",
       "  'sentence': 'What would the old town of Graz be without the wonderful inner courtyards? What would a summer in Graz be without jazz concerts in the Generalihof? Every year in summer, the Generalihof is a meeting place for connoisseurs: every evening, 100 guests have the opportunity to enjoy the ambience, the Styrian wine and the great jazz program in the Generalihof.'},\n",
       " {'url': 'https://www.graztourismus.at/en/sightseeing-culture/sights/hof-des-deutschen-ritterordens-courtyard_shg_6955',\n",
       "  'id': '<urn:uuid:62be352d-1a96-49c9-b2b1-d465c0d8f4bc>',\n",
       "  'title': 'No Tittle',\n",
       "  'language': 'en',\n",
       "  'warcDate': '1684768609000000',\n",
       "  'wordCount': 220,\n",
       "  'sentence': 'The Gothic arcaded courtyard of the “Hof des deutschen Ritterordens” is definitely worth a visit. It is paved entirely with “Murnockerl”, the Graz name for the cobblestones smoothed round by the river Mur, which were used as paving material for streets and courtyards.'},\n",
       " {'url': 'https://www.graztourismus.at/en/sightseeing-culture/sights/luegghaus_shg_6953',\n",
       "  'id': '<urn:uuid:52c17a46-8e5d-4127-99b9-62b321345d4f>',\n",
       "  'title': 'No Tittle',\n",
       "  'language': 'en',\n",
       "  'warcDate': '1684768605000000',\n",
       "  'wordCount': 229,\n",
       "  'sentence': 'Amongst the superb façades around Hauptplatz main square, the “Luegghaus” on the corner of Sporgasse is particularly striking thanks to its elaborate stucco façade and arcades. The mouths and noses hidden among fruit and flower garlands on the façade will not escape attentive beholders.'},\n",
       " {'url': 'https://www.graztourismus.at/en/sightseeing-culture/sights/argos-by-zaha-hadid-architects_shg_7655',\n",
       "  'id': '<urn:uuid:aada31a6-7a74-4081-86fe-b20f57538e13>',\n",
       "  'title': 'No Tittle',\n",
       "  'language': 'en',\n",
       "  'warcDate': '1684768600000000',\n",
       "  'wordCount': 247,\n",
       "  'sentence': 'Planned by the exceptional architect Zaha Hadid and realised by the project developer WEGRAZ, ARGOS is a vision, concept, backdrop and home. For everyone seeking something special. For those wishing to escape the everyday for a little while and for those who have come to stay longer. ARGOS combines the comfort of having “your own four walls” with the service of a hotel. ARGOS provides magical encounters with design, experiences and life.'},\n",
       " {'url': 'https://www.graztourismus.at/en/sightseeing-culture/sights/mumuth_shg_7022',\n",
       "  'id': '<urn:uuid:76033e1a-9c05-4496-a8db-1d902ff70f38>',\n",
       "  'title': 'No Tittle',\n",
       "  'language': 'en',\n",
       "  'warcDate': '1684768599000000',\n",
       "  'wordCount': 271,\n",
       "  'sentence': 'The House of Music and Music Theatre (MUMUTH) was built specially for KUG. It was first used in 2009 after a three-year construction period. The spectacular design by Dutch architect, Ben van Berkel, is based on the winning project from an international competition, and is dominated by steel, concrete and glass in various combinations and superimpositions. In 2010 it was awarded the Fischer-von-Erlach Prize and the Urban Land Institute Award. Alongside the large György-Ligeti-Saal, with its unique, variable acoustics, it offers an orchestra rehearsal space and a rehearsal stage, as well as other studios and theatrical infrastructure.'},\n",
       " {'url': 'https://www.graztourismus.at/en/sightseeing-culture/sights/gosting-castle-ruins_shg_1446',\n",
       "  'id': '<urn:uuid:d32569ff-30e6-42f9-bfd1-031655fab938>',\n",
       "  'title': 'No Tittle',\n",
       "  'language': 'en',\n",
       "  'warcDate': '1684768618000000',\n",
       "  'wordCount': 287,\n",
       "  'sentence': 'Despairing maidens and precipitous views. The ruins of Gösting castle offer the perfect destination for a trek within easy reach of the city. The steep but short ascent passes by the ‘Jungfernsprung’, the place from which, legend has it, the lovesick and grief-stricken Anna von Gösting threw herself to her doom. Further up, by the castle ruins, there are impressive views of\\xa0 the strategically important valley of the river Mur, Graz itself and the landscape around Gösting.'},\n",
       " {'url': 'https://www.graztourismus.at/en/sightseeing-culture/sights/schlossberg-lift_shg_1484',\n",
       "  'id': '<urn:uuid:ada412d6-cfa2-40a6-babe-963089df3ff1>',\n",
       "  'title': 'No Tittle',\n",
       "  'language': 'en',\n",
       "  'warcDate': '1684768604000000',\n",
       "  'wordCount': 330,\n",
       "  'sentence': 'In the fast lane. An unusual, fast and yet comfortable way to achieve the ascent of Schlossberg hill in Graz is the Schlossberg lift. The altogether aesthetically appealing design of this new lift, set into the very core of the hill, is also particularly practical if you’re in a hurry or not inclined or able to walk. During the ride, the lift’s glass cabins offer a clear view of the illuminated, rock-lined elevator shaft and distant sky above – a mountain view from within.'},\n",
       " {'url': 'https://www.graztourismus.at/en/sightseeing-culture/sights/hofbackerei-edegger-tax_shg_6959',\n",
       "  'id': '<urn:uuid:c4c82c7b-bc62-4ce5-8fd3-396a2e79ea71>',\n",
       "  'title': 'No Tittle',\n",
       "  'language': 'en',\n",
       "  'warcDate': '1684768605000000',\n",
       "  'wordCount': 383,\n",
       "  'sentence': 'In 1787 Mathias Tax bought the bakery and in 1880 Franz Tax III moved it to its current location and made it a prestigious bakery. During the visit of Emperor Franz Joseph to Graz in 1883, the bakery supplied the royal court. Due to the quality of its products, the bakery received an Imperial and Royal Warrant of Appointment in 1888. The carved wooden shop portal was made by the carpenter Anton Irschik from Graz in 1896. In 1940s Franz Edegger married Herta Tax and they ran the bakery together. After that, Erich Edegger and his wife Walheide continued the tradition of the bakery. Today the company is run by his son Robert Edegger and his wife Brigitte.'},\n",
       " {'url': 'https://www.graztourismus.at/en/sightseeing-culture/sights/schlossberg-funicular_shg_1482',\n",
       "  'id': '<urn:uuid:d96ca59f-b162-432d-a937-adeacd073c5c>',\n",
       "  'title': 'No Tittle',\n",
       "  'language': 'en',\n",
       "  'warcDate': '1684768601000000',\n",
       "  'wordCount': 414,\n",
       "  'sentence': 'What do Graz and Niagara Falls have in common? True, not easy to guess – it’s a cable funicular railway. The Schlossbergbahn cabins carry visitors up and down the central Schlossberg\\xa0hill of Graz in just a minute and a half. Riding the steep incline of the track offers the odd thrill and wonderful views of Graz. For over one hundred years this spectacular funicular has been conveying people up to Schlossberg hill in style, with a view to taking in the panorama and perhaps some giddy festivities.'}]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8000/search?q=graz&index=demo-graz&lang=en&ranking=asc&limit=10\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Ensure the request was successful\n",
    "if response.status_code == 200:\n",
    "    data2 = response.json()['results']\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "for item in data2:\n",
    "    item['sentence'] = item.pop('textSnippet')\n",
    "    item['title'] = 'No Tittle'\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"president\": \"Joe Biden\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "completion = client.completions.create(model='mistral', \n",
    "    prompt=f\"Q: who is the president of USA. Send result in json format\\nA:\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=100,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    stop=[\"\\n\"])\n",
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<iframe src=\"https://en.wikipedia.org/wiki/File:Innere_Stadt,_8010_Graz,_Austria_-_panoramio_(22).jpg\" width=\"700\" height=\"450\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import Ollama\n",
    "from llama_index.llms import ChatMessage\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import LLMPredictor, ServiceContext\n",
    "\n",
    "LLM = Ollama(model=\"mistral\")\n",
    "service_context2 = ServiceContext.from_defaults(llm=LLM,embed_model=\"local\")\n",
    "\n",
    "# messages = [\n",
    "#     ChatMessage(\n",
    "#         role=\"system\", content=\"You are a cat with a colorful personality\"\n",
    "#     ),\n",
    "#     ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "# ]\n",
    "callback_handler = OpenInferenceCallbackHandler()\n",
    "callback_manager = CallbackManager([callback_handler])\n",
    "llm_predictor = LLMPredictor(llm=LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098be1071d644c4e87e1ef9b7bd7dca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44e3e9abe1b4499a72ba36f4124ea81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parser = SimpleNodeParser.from_defaults(chunk_size=1024)\n",
    "nodes = parser.get_nodes_from_documents(docs,show_progress=True)\n",
    "print(\"Building index...\")\n",
    "index = VectorStoreIndex(\n",
    "    nodes, service_context=service_context2,show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index import (\n",
    "    Prompt,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "response_template = \"\"\"\n",
    "## Question\n",
    "\n",
    "{question}\n",
    "\n",
    "\n",
    "## Answer\n",
    "```\n",
    "{response}\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "template = (\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, please answer the question: {query_str}\\n\"\n",
    "    \"Find information from different urls. \\n\"\n",
    "    \"Find at least one photo url with related query. \\n\"\n",
    "    \"Find top 10 distinct related url which related to query. Rename field as results. \\n\"\n",
    "    \"Reduce each document to url and title and one small sentence and if photo url exits put photo url to the json. Rename snippet to sentence. \\n\"\n",
    "    \"Result should includes url, tittle, sentence.\"\n",
    "    \"Return the response in json format.\\n\"\n",
    "    \"Do not return in markdown format, return in json format.\\n\"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "response_synthesizer = get_response_synthesizer(text_qa_template= qa_template,service_context=service_context2)\n",
    "query_engine2 = index.as_query_engine(similarity_top_k = 3,response_synthesizer=response_synthesizer)\n",
    "\n",
    "def ask(query: str, top_k: int = 12):\n",
    "    \n",
    "    response_md = query_engine2.query(query)\n",
    "    display(Markdown(response_template.format(\n",
    "        question=query,\n",
    "        response=response_md,\n",
    "    \n",
    "    )))\n",
    "    return response_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ask(\u001b[39m\"\u001b[39;49m\u001b[39mGraz\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X24sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mask\u001b[39m(query: \u001b[39mstr\u001b[39m, top_k: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m12\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X24sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     response_md \u001b[39m=\u001b[39m query_engine2\u001b[39m.\u001b[39;49mquery(query)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X24sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     display(Markdown(response_template\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X24sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m         question\u001b[39m=\u001b[39mquery,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X24sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         response\u001b[39m=\u001b[39mresponse_md,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X24sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X24sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     )))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ngumus/Desktop/hackhaton/demos/web_index.ipynb#X24sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response_md\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/core/base_query_engine.py:30\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     29\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 30\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/query_engine/retriever_query_engine.py:171\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    168\u001b[0m     CBEventType\u001b[39m.\u001b[39mQUERY, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    169\u001b[0m ) \u001b[39mas\u001b[39;00m query_event:\n\u001b[1;32m    170\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 171\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_synthesizer\u001b[39m.\u001b[39;49msynthesize(\n\u001b[1;32m    172\u001b[0m         query\u001b[39m=\u001b[39;49mquery_bundle,\n\u001b[1;32m    173\u001b[0m         nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    176\u001b[0m     query_event\u001b[39m.\u001b[39mon_end(payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py:146\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m     query \u001b[39m=\u001b[39m QueryBundle(query_str\u001b[39m=\u001b[39mquery)\n\u001b[1;32m    143\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    144\u001b[0m     CBEventType\u001b[39m.\u001b[39mSYNTHESIZE, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    145\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m--> 146\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m    147\u001b[0m         query_str\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mquery_str,\n\u001b[1;32m    148\u001b[0m         text_chunks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    149\u001b[0m             n\u001b[39m.\u001b[39;49mnode\u001b[39m.\u001b[39;49mget_content(metadata_mode\u001b[39m=\u001b[39;49mMetadataMode\u001b[39m.\u001b[39;49mLLM) \u001b[39mfor\u001b[39;49;00m n \u001b[39min\u001b[39;49;00m nodes\n\u001b[1;32m    150\u001b[0m         ],\n\u001b[1;32m    151\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    154\u001b[0m     additional_source_nodes \u001b[39m=\u001b[39m additional_source_nodes \u001b[39mor\u001b[39;00m []\n\u001b[1;32m    155\u001b[0m     source_nodes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nodes) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/compact_and_refine.py:38\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m new_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m     39\u001b[0m     query_str\u001b[39m=\u001b[39;49mquery_str,\n\u001b[1;32m     40\u001b[0m     text_chunks\u001b[39m=\u001b[39;49mnew_texts,\n\u001b[1;32m     41\u001b[0m     prev_response\u001b[39m=\u001b[39;49mprev_response,\n\u001b[1;32m     42\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m     43\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py:127\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks:\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m prev_response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[39m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         \u001b[39m# is an answer, then return it\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_give_response_single(\n\u001b[1;32m    128\u001b[0m             query_str, text_chunk, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs\n\u001b[1;32m    129\u001b[0m         )\n\u001b[1;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[39m# refine response if possible\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_refine_response_single(\n\u001b[1;32m    133\u001b[0m             prev_response, query_str, text_chunk, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kwargs\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py:182\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streaming:\n\u001b[1;32m    179\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m         structured_response \u001b[39m=\u001b[39m cast(\n\u001b[1;32m    181\u001b[0m             StructuredRefineResponse,\n\u001b[0;32m--> 182\u001b[0m             program(\n\u001b[1;32m    183\u001b[0m                 context_str\u001b[39m=\u001b[39;49mcur_text_chunk,\n\u001b[1;32m    184\u001b[0m                 output_cls\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_output_cls,\n\u001b[1;32m    185\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs,\n\u001b[1;32m    186\u001b[0m             ),\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m         query_satisfied \u001b[39m=\u001b[39m structured_response\u001b[39m.\u001b[39mquery_satisfied\n\u001b[1;32m    189\u001b[0m         \u001b[39mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py:53\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StructuredRefineResponse:\n\u001b[0;32m---> 53\u001b[0m     answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm_predictor\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m     54\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prompt,\n\u001b[1;32m     55\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[39m=\u001b[39manswer, query_satisfied\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/llm_predictor/base.py:224\u001b[0m, in \u001b[0;36mLLMPredictor.predict\u001b[0;34m(self, prompt, output_cls, **prompt_args)\u001b[0m\n\u001b[1;32m    222\u001b[0m     formatted_prompt \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat(llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[1;32m    223\u001b[0m     formatted_prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_prompt(formatted_prompt)\n\u001b[0;32m--> 224\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm\u001b[39m.\u001b[39;49mcomplete(formatted_prompt)\n\u001b[1;32m    225\u001b[0m     output \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mtext\n\u001b[1;32m    227\u001b[0m logger\u001b[39m.\u001b[39mdebug(output)\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/llms/base.py:313\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mwith\u001b[39;00m wrapper_logic(_self) \u001b[39mas\u001b[39;00m callback_manager:\n\u001b[1;32m    304\u001b[0m     event_id \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m    305\u001b[0m         CBEventType\u001b[39m.\u001b[39mLLM,\n\u001b[1;32m    306\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m         },\n\u001b[1;32m    311\u001b[0m     )\n\u001b[0;32m--> 313\u001b[0m     f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    315\u001b[0m         \u001b[39m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_gen\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CompletionResponseGen:\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/llms/ollama.py:130\u001b[0m, in \u001b[0;36mOllama.complete\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@llm_completion_callback\u001b[39m()\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomplete\u001b[39m(\u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CompletionResponse:\n\u001b[1;32m    129\u001b[0m     response_gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_complete(prompt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 130\u001b[0m     response_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(response_gen)\n\u001b[1;32m    131\u001b[0m     final_response \u001b[39m=\u001b[39m response_list[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    132\u001b[0m     final_response\u001b[39m.\u001b[39mdelta \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/llms/base.py:318\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict.<locals>.wrapped_gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_gen\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CompletionResponseGen:\n\u001b[1;32m    317\u001b[0m     last_response \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m     \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m f_return_val:\n\u001b[1;32m    319\u001b[0m         \u001b[39myield\u001b[39;49;00m cast(CompletionResponse, x)\n\u001b[1;32m    320\u001b[0m         last_response \u001b[39m=\u001b[39;49m x\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/llama_index/llms/ollama.py:162\u001b[0m, in \u001b[0;36mOllama.stream_complete.<locals>.gen\u001b[0;34m(response_iter)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgen\u001b[39m(response_iter: Iterator[Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CompletionResponseGen:\n\u001b[1;32m    161\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mfor\u001b[39;49;00m stream_response \u001b[39min\u001b[39;49;00m response_iter:\n\u001b[1;32m    163\u001b[0m         delta \u001b[39m=\u001b[39;49m json\u001b[39m.\u001b[39;49mloads(stream_response)\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mresponse\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    164\u001b[0m         text \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m delta\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[39m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[39m=\u001b[39;49mchunk_size, decode_unicode\u001b[39m=\u001b[39;49mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[39mif\u001b[39;49;00m pending \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[39m=\u001b[39;49m pending \u001b[39m+\u001b[39;49m chunk\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/requests/utils.py:571\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    570\u001b[0m decoder \u001b[39m=\u001b[39m codecs\u001b[39m.\u001b[39mgetincrementaldecoder(r\u001b[39m.\u001b[39mencoding)(errors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m iterator:\n\u001b[1;32m    572\u001b[0m     rv \u001b[39m=\u001b[39;49m decoder\u001b[39m.\u001b[39;49mdecode(chunk)\n\u001b[1;32m    573\u001b[0m     \u001b[39mif\u001b[39;49;00m rv:\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/urllib3/response.py:931\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[39mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[39m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 931\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content)\n\u001b[1;32m    932\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/urllib3/response.py:1071\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1071\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_chunk_length()\n\u001b[1;32m   1072\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1073\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/hackhaton/.venv/lib/python3.11/site-packages/urllib3/response.py:999\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    998\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline()  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1001\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ask(\"Graz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from llama_index.llms import Ollama\n",
    "from llama_index.llms import ChatMessage\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import LLMPredictor, ServiceContext\n",
    "\n",
    "LLM = Ollama(model=\"mistral\")\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = oembed.embed_documents(\n",
    "            [\n",
    "                \"Alpha is the first letter of Greek alphabet\",\n",
    "                \"Beta is the second letter of Greek alphabet\",\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2454b281c1b245a98498fc6534f2271b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec022cfd3bb428599584bbf4be5d274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.schema import Document\n",
    "ds = [\n",
    "    Document(doc_id= \"1\", text=\"Alpha is the first letter of Greek alphabet\",metadata={\"name\":\"doc1\"} ),\n",
    "    Document(doc_id= \"2\", text=\"Beta is the second letter of Greek alphabet\",metadata={\"name\":\"doc2\"} )\n",
    "    \n",
    "]\n",
    "service_context3 = ServiceContext.from_defaults(llm=LLM,embed_model=oembed)\n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=1024)\n",
    "nodes = parser.get_nodes_from_documents(ds,show_progress=True)\n",
    "print(\"Building index...\")\n",
    "index = VectorStoreIndex(\n",
    "    nodes, service_context=service_context3, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDelta is the fourth letter of Greek alphabet.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(index.as_query_engine().query(\"Delta\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graz, city in Austria <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/17-05-16-Graz_Rathaus-aDSC_1282.jpg/2560px-17-05-16-Graz_Rathaus-aDSC_1282.jpg\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
